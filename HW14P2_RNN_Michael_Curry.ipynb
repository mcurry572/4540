{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcurry572/4540/blob/main/HW14P2_RNN_Michael_Curry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab tutorial for uploading data.\n",
        "\n",
        "https://colab.research.google.com/notebooks/io.ipynb"
      ],
      "metadata": {
        "id": "0V_KuxkcZv4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "filepath = 'Rat08-20130711_017.h5'  # data file\n",
        "f = h5py.File(filepath, 'r')  # read data with h5 format\n",
        "fs = f.attrs['fs'][0]  # get sampling frequency of LFP signal (Hz)\n",
        "print(\"Sampling rate: %.1f Hz\" % (fs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Zbu56lbcZy9U",
        "outputId": "4a7e4c6f-89a6-4ebe-ca5e-0ed717d65594"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Unable to synchronously open file (truncated file: eof = 1048576, sblock->base_addr = 0, stored_eof = 46559900)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0b10b060ed2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Rat08-20130711_017.h5'\u001b[0m  \u001b[0;31m# data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# read data with h5 format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# get sampling frequency of LFP signal (Hz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sampling rate: %.1f Hz\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to synchronously open file (truncated file: eof = 1048576, sblock->base_addr = 0, stored_eof = 46559900)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = []  # two states ,NREM & WAKE, need to be classified\n",
        "# LFP recordings are store in two h5 groups for each state\n",
        "# in each h5 group, the LFP recordings are divided into several segments with different lengths.\n",
        "for name, grp in f.items():\n",
        "  states.append(name)\n",
        "  print(\"State: %s\" % (name))\n",
        "  print(\"Segment IDs:\")\n",
        "  print(list(grp.keys()))"
      ],
      "metadata": {
        "id": "c-tsFnkF4ikk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IoqxVilXqr_"
      },
      "outputs": [],
      "source": [
        "# Use a dictionary to store the LFP recordings of the two states\n",
        "# each containing a list of numpy arrays of all segments\n",
        "lfp = {key: [] for key in states}\n",
        "for key in states:\n",
        "  group = f[key]\n",
        "  n = len(group)\n",
        "  for i in range(n):\n",
        "    lfp[key].append(group[str(i+1)][()].astype(float))  # convert data to numpy array and from int type to float type\n",
        "\n",
        "# print(lfp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example for accessing a segment."
      ],
      "metadata": {
        "id": "bd_dM9kPE4jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = lfp['NREM'][10]  # accessing the 10-th LFP segment in NREM state\n",
        "t = np.arange(x.size)/fs  # time points\n",
        "\n",
        "plt.plot(t,x)\n",
        "plt.xlabel('second')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L_Bo9VG28e6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LFP State Classification with a Fully Connected Neural Network\n"
      ],
      "metadata": {
        "id": "5SbH3Uesv0NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Import Libraries\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import welch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(18)\n",
        "torch.manual_seed(18)\n"
      ],
      "metadata": {
        "id": "y8ZRLJYSvxXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "TzcxPYz2v9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads the H5 file and returns a dictionary where each key corresponds to a brain state (e.g., NREM, WAKE)\n",
        "    and the associated value is a numpy array of the signal.\n",
        "\n",
        "    If the key corresponds to a group, this function attempts to load either a dataset named 'data'\n",
        "    (if available) or the first dataset found in the group.\n",
        "    \"\"\"\n",
        "    import h5py  # ensure h5py is imported within function if not already\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        keys = list(f.keys())\n",
        "        print(\"Keys found in {}: {}\".format(file_path, keys))\n",
        "        state_data = {}\n",
        "        for key in keys:\n",
        "            item = f[key]\n",
        "            # Check if the item is a dataset\n",
        "            if isinstance(item, h5py.Dataset):\n",
        "                state_data[key] = item[:]   # load the dataset directly\n",
        "            elif isinstance(item, h5py.Group):\n",
        "                sub_keys = list(item.keys())\n",
        "                print(\"Key '{}' is a group with sub-keys: {}\".format(key, sub_keys))\n",
        "                # Look for a dataset named 'data'\n",
        "                if 'data' in sub_keys:\n",
        "                    state_data[key] = item['data'][:]\n",
        "                elif len(sub_keys) > 0:\n",
        "                    # If no 'data' key exists, load the first dataset available in the group\n",
        "                    first_key = sub_keys[0]\n",
        "                    state_data[key] = item[first_key][:]\n",
        "                else:\n",
        "                    raise ValueError(\"Group '{}' is empty in file {}.\".format(key, file_path))\n",
        "            else:\n",
        "                raise TypeError(\"Key {} is neither a Group nor a Dataset.\".format(key))\n",
        "    return state_data\n",
        "\n",
        "# List of file paths (update with correct paths if necessary)\n",
        "file_paths = [\n",
        "    'Part1SubjectHB10.h5',\n",
        "    'Rat08-20130711_017.h5',\n",
        "    'Part2SubjectHB13.h5'\n",
        "]\n",
        "\n",
        "# Load data from the files into a list of dictionaries.\n",
        "all_subjects_data = [load_data(fp) for fp in file_paths]\n"
      ],
      "metadata": {
        "id": "PsPZ3kb5v7_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and Feature Extraction\n",
        "\n",
        "### 1. Segmenting the Signal\n",
        "We segment each recording into non-overlapping clips of fixed length (e.g. 5 seconds).  \n",
        "We assume the sampling frequency is fs = 1000 Hz (i.e. 1000 samples per second).\n",
        "\n",
        "### 2. Extracting Features\n",
        "For each clip we extract time-domain features (mean, standard deviation, skewness, kurtosis) and frequency-domain features. In the frequency domain we compute the power spectral density (PSD) using Welch’s method and extract:\n",
        "- Total power\n",
        "- Relative power in classic frequency bands:\n",
        "  - Delta (0.5–4 Hz)\n",
        "  - Theta (4–8 Hz)\n",
        "  - Alpha (8–12 Hz)\n",
        "  - Beta (12–30 Hz)\n",
        "\n",
        "These features are concatenated to form a feature vector per clip.\n"
      ],
      "metadata": {
        "id": "fhQ4TKLTwHNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Define Signal Segmentation and Feature Extraction\n",
        "\n",
        "fs = 1000                # Sampling frequency in Hz\n",
        "clip_length_sec = 5      # Duration of each clip in seconds\n",
        "clip_length_samples = fs * clip_length_sec\n",
        "\n",
        "def segment_signal(signal, clip_length_samples):\n",
        "    \"\"\"\n",
        "    Segments the signal into non-overlapping clips.\n",
        "    \"\"\"\n",
        "    n_samples = len(signal)\n",
        "    clips = []\n",
        "    # Step in non-overlapping windows\n",
        "    for start in range(0, n_samples - clip_length_samples + 1, clip_length_samples):\n",
        "        clip = signal[start:start+clip_length_samples]\n",
        "        clips.append(clip)\n",
        "    return clips\n",
        "\n",
        "def extract_features(clip, fs):\n",
        "    \"\"\"\n",
        "    Given a clip, compute time-domain and frequency-domain features.\n",
        "    Returns a feature vector.\n",
        "    \"\"\"\n",
        "    # Time-domain features\n",
        "    mean_val = np.mean(clip)\n",
        "    std_val = np.std(clip)\n",
        "    skew_val = skew(clip)\n",
        "    kurt_val = kurtosis(clip)\n",
        "\n",
        "    # Frequency-domain: compute PSD using Welch's method.\n",
        "    f_vals, pxx = welch(clip, fs=fs, nperseg=fs//2)\n",
        "\n",
        "    # Total power of the signal\n",
        "    total_power = np.sum(pxx) + 1e-9  # add small number to avoid division by zero\n",
        "\n",
        "    # Helper function: power in a frequency band\n",
        "    def band_power(f, pxx, low, high):\n",
        "        idx = np.logical_and(f >= low, f < high)\n",
        "        return np.sum(pxx[idx])\n",
        "\n",
        "    delta_power = band_power(f_vals, pxx, 0.5, 4)\n",
        "    theta_power = band_power(f_vals, pxx, 4, 8)\n",
        "    alpha_power = band_power(f_vals, pxx, 8, 12)\n",
        "    beta_power = band_power(f_vals, pxx, 12, 30)\n",
        "\n",
        "    # Compute relative band powers\n",
        "    delta_rel = delta_power / total_power\n",
        "    theta_rel = theta_power / total_power\n",
        "    alpha_rel = alpha_power / total_power\n",
        "    beta_rel = beta_power / total_power\n",
        "\n",
        "    # Concatenate all features: time-domain features, total power, and relative band powers.\n",
        "    features = np.array([\n",
        "        mean_val, std_val, skew_val, kurt_val,\n",
        "        total_power, delta_rel, theta_rel, alpha_rel, beta_rel\n",
        "    ])\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "C8i4feV4wIHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Feature Dataset\n",
        "\n",
        "Next, we loop over the subjects and for each state (NREM and WAKE) we segment the raw LFP signal into clips, compute the features, and assign labels:\n",
        "- Label 0: NREM  \n",
        "- Label 1: WAKE  \n",
        "\n",
        "After combining all clips, we normalize the features and split the dataset into training and testing sets.\n"
      ],
      "metadata": {
        "id": "KV4kTWTxwJpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Create Dataset for Classification\n",
        "\n",
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "# For each subject file\n",
        "for subject_data in all_subjects_data:\n",
        "    for state, signal in subject_data.items():\n",
        "        # Check that the signal is one-dimensional (if not, select the desired channel)\n",
        "        if signal.ndim > 1:\n",
        "            signal = signal.flatten()\n",
        "\n",
        "        # Segment the signal into clips\n",
        "        clips = segment_signal(signal, clip_length_samples)\n",
        "        print(\"Number of clips for state {}: {}\".format(state, len(clips)))\n",
        "\n",
        "        # Extract features for each clip\n",
        "        for clip in clips:\n",
        "            feat = extract_features(clip, fs)\n",
        "            X_all.append(feat)\n",
        "            # Assign a label based on state name\n",
        "            if state.upper() == 'NREM':\n",
        "                y_all.append(0)\n",
        "            elif state.upper() == 'WAKE':\n",
        "                y_all.append(1)\n",
        "            else:\n",
        "                # non-standard state names\n",
        "                continue\n",
        "\n",
        "X_all = np.array(X_all)\n",
        "y_all = np.array(y_all)\n",
        "\n",
        "print(\"Total samples: {}, Feature vector length: {}\".format(X_all.shape[0], X_all.shape[1]))\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_all_norm = scaler.fit_transform(X_all)\n",
        "\n",
        "# Split dataset into training and test sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all_norm, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader objects for batching\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Gpy6G_uBwMcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Neural Network with PyTorch\n",
        "\n",
        "We define a simple feedforward neural network with one hidden layer. The network uses ReLU activation and has an output layer with two neurons corresponding to the two classes.\n"
      ],
      "metadata": {
        "id": "pRo0ybQ7wOI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define the Model\n",
        "\n",
        "class LFPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(LFPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 2)  # 2 classes: NREM and WAKE\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "input_dim = X_all_norm.shape[1]\n",
        "model = LFPClassifier(input_dim)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "Gr7JJDVcwOkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "We set up the loss function (cross-entropy) and the Adam optimizer. Then we train the network over a number of epochs while tracking the training loss. We also evaluate the model on the test set to monitor its performance.\n"
      ],
      "metadata": {
        "id": "-vy6NnncxUMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Training Loop\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "n_epochs = 100\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            outputs = model(batch_x)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "    test_accuracy = correct / total\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(\"Epoch [{}/{}]: Loss = {:.4f}, Test Accuracy = {:.4f}\".format(epoch+1, n_epochs, epoch_loss, test_accuracy))\n"
      ],
      "metadata": {
        "id": "AjyvcjPdxPNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "Yzdr7RinwQ2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Plot Training Loss and Test Accuracy\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax1.plot(train_losses, color=color)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Training Loss', color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "ax1.set_title('Training Loss and Test Accuracy Over Epochs')\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "color = 'tab:red'\n",
        "ax2.plot(test_accuracies, color=color)\n",
        "ax2.set_ylabel('Test Accuracy', color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R7WpG-ovwVBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data"
      ],
      "metadata": {
        "id": "hGwQ4CY9Na7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Prepare & Load Short Clips for CNN (memory‐safe)\n",
        "\n",
        "import h5py, numpy as np, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 1) Load H5, get fs and all segments\n",
        "filepath = 'Rat08-20130711_017.h5'\n",
        "with h5py.File(filepath, 'r') as f:\n",
        "    fs = int(f.attrs['fs'][0])\n",
        "    raw_states = {\n",
        "        state: [f[state][k][()].astype(np.float32)\n",
        "                for k in f[state].keys()]\n",
        "        for state in f.keys()\n",
        "    }\n",
        "\n",
        "# 2) Define clip length in seconds & samples\n",
        "clip_sec = 2\n",
        "clip_len = fs * clip_sec    # e.g. 3000 samples if fs=1000\n",
        "\n",
        "# 3) Slide non‐overlapping window to create fixed‐length clips\n",
        "all_X, all_y = [], []\n",
        "label_map = {'NREM':0, 'WAKE':1}\n",
        "\n",
        "for state, segs in raw_states.items():\n",
        "    for sig in segs:\n",
        "        n = sig.shape[0]\n",
        "        # how many full clips fit?\n",
        "        n_clips = n // clip_len\n",
        "        for i in range(n_clips):\n",
        "            start = i * clip_len\n",
        "            clip  = sig[start:start+clip_len]\n",
        "            all_X.append(clip)\n",
        "            all_y.append(label_map[state])\n",
        "\n",
        "all_X = np.stack(all_X, axis=0)  # (N_clips, clip_len)\n",
        "all_y = np.array(all_y, dtype=np.int64)\n",
        "\n",
        "print(\"Total clips:\", all_X.shape[0], \"Clip length:\", all_X.shape[1])\n",
        "\n",
        "# 4) Dataset + DataLoaders\n",
        "class LFPClipDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).unsqueeze(1)  # → (N,1,clip_len)\n",
        "        self.y = torch.from_numpy(y)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "dataset = LFPClipDataset(all_X, all_y)\n",
        "\n",
        "# 5) Train/test split\n",
        "n = len(dataset)\n",
        "idx = np.arange(n)\n",
        "np.random.seed(42); np.random.shuffle(idx)\n",
        "split = int(0.8 * n)\n",
        "train_idx, test_idx = idx[:split], idx[split:]\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "train_ds = Subset(dataset, train_idx)\n",
        "test_ds  = Subset(dataset, test_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds,  batch_size=8, shuffle=True,  pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, pin_memory=True)\n",
        "\n",
        "print(\"Batches – train:\", len(train_loader), \"test:\", len(test_loader))\n"
      ],
      "metadata": {
        "id": "MMk-Z1hyJsBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a 1D-CNN Model**"
      ],
      "metadata": {
        "id": "IFCZujh_NiSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LFP_CNN(nn.Module):\n",
        "    def __init__(self, seq_len):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, padding=2)\n",
        "        self.pool  = nn.MaxPool1d(2)\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)\n",
        "        # after two pool-by-2’s, length = seq_len // 4\n",
        "        self.fc1   = nn.Linear(32 * (seq_len // 4), 64)\n",
        "        self.fc2   = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,1,L)\n",
        "        x = self.pool(F.relu(self.conv1(x)))   # → (B,16,L/2)\n",
        "        x = self.pool(F.relu(self.conv2(x)))   # → (B,32,L/4)\n",
        "        x = x.flatten(1)                       # → (B, 32*(L/4))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model_cnn = LFP_CNN(seq_len = X_pad.shape[1]).to(device)\n",
        "model_cnn = LFP_CNN(seq_len=clip_len).to(device)\n",
        "print(model_cnn)\n"
      ],
      "metadata": {
        "id": "Gcc2uYsYNnbW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "72920a65-6224-4bf0-c390-39be934f558b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ebe38afaf740>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#model_cnn = LFP_CNN(seq_len = X_pad.shape[1]).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLFP_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Evaluate"
      ],
      "metadata": {
        "id": "jUXCFwVPNpW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_cnn.parameters(), lr=1e-3)\n",
        "\n",
        "def train_epoch(model, loader):\n",
        "    model.train()\n",
        "    running_loss, N = 0.0, 0\n",
        "    for Xb, yb in loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * Xb.size(0)\n",
        "        N += Xb.size(0)\n",
        "    return running_loss / N\n",
        "\n",
        "def eval_model(model, loader):\n",
        "    model.eval()\n",
        "    correct, N = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            preds = model(Xb).argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            N += yb.size(0)\n",
        "    return correct / N\n",
        "\n",
        "# --- CNN training with metric recording ---\n",
        "cnn_train_losses, cnn_test_accuracies = [], []\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    tr_loss = train_epoch(model_cnn, train_loader)\n",
        "    te_acc  = eval_model(model_cnn, test_loader)\n",
        "    cnn_train_losses.append(tr_loss)\n",
        "    cnn_test_accuracies.append(te_acc)\n",
        "    print(f\"[CNN] Epoch {epoch}/{n_epochs} — loss: {tr_loss:.4f}, test acc: {te_acc:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oAzjs7NpNrwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CNN: Training Loss\n",
        "plt.figure()\n",
        "plt.plot(cnn_train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('CNN Training Loss over Epochs')\n",
        "plt.show()\n",
        "\n",
        "# CNN: Test Accuracy\n",
        "plt.figure()\n",
        "plt.plot(cnn_test_accuracies)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('CNN Test Accuracy over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "em45KiUtYVBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An LSTM (Long Short-Term Memory) network is a specialized type of Recurrent Neural Network designed to learn long term dependencies through the use of gated mechanisms.\n",
        "\n",
        "- The input gate controls which new information is allowed into the cell state.\n",
        "\n",
        "- The forget gate determines what information should be discarded, helping manage long-term memories.\n",
        "\n",
        "- The output gate decides what information from the cell should be passed to the next layer.\n",
        "\n",
        "- LSTMs maintain a cell state that allows information to flow across many time steps.\n",
        "\n",
        "- By introducing this memory component, LSTMs solve the problem of exploding and or vanishing gradients that traditional RNNs face.\n",
        "\n",
        "- In this demonstration I sequentially feed each timepoint of the LFP signal into the LSTM to capture its temporal dynamics for accurate state classification."
      ],
      "metadata": {
        "id": "YrxckDsNNvaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining LSTM Model"
      ],
      "metadata": {
        "id": "-w3i3IzON-tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LFP_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            batch_first=True, bidirectional=False)\n",
        "        self.fc   = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B,1,L] → permute to [B,L,1]\n",
        "        x = x.permute(0,2,1)\n",
        "        out, _ = self.lstm(x)           # out: [B, L, hidden_size]\n",
        "        # take last time step\n",
        "        last = out[:, -1, :]            # [B, hidden_size]\n",
        "        return self.fc(last)\n",
        "\n",
        "model_lstm = LFP_LSTM().to(device)\n",
        "optimizer_l = optim.Adam(model_lstm.parameters(), lr=1e-3)\n",
        "criterion_l = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "AfhTH_WKJsmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Evalaute"
      ],
      "metadata": {
        "id": "ke77rDsZOBuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_lstm(model, loader):\n",
        "    model.train()\n",
        "    total_loss, count = 0,0\n",
        "    for Xb,yb in loader:\n",
        "        Xb,yb = Xb.to(device), yb.to(device)\n",
        "        optimizer_l.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss   = criterion_l(logits, yb)\n",
        "        loss.backward(); optimizer_l.step()\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "        count += Xb.size(0)\n",
        "    return total_loss/count\n",
        "\n",
        "def eval_lstm(model, loader):\n",
        "    model.eval()\n",
        "    correct, total= 0,0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb,yb = Xb.to(device), yb.to(device)\n",
        "            preds = model(Xb).argmax(dim=1)\n",
        "            correct += (preds==yb).sum().item()\n",
        "            total   += yb.size(0)\n",
        "    return correct/total\n",
        "\n",
        "# --- LSTM training with metric recording ---\n",
        "lstm_train_losses, lstm_test_accuracies = [], []\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    tr_loss = train_epoch_lstm(model_lstm, train_loader)\n",
        "    te_acc  = eval_lstm(model_lstm, test_loader)\n",
        "    lstm_train_losses.append(tr_loss)\n",
        "    lstm_test_accuracies.append(te_acc)\n",
        "    print(f\"[LSTM] Epoch {epoch}/{n_epochs} — loss: {tr_loss:.4f}, test acc: {te_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "sMSKTbAqODs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# LSTM: Training Loss\n",
        "plt.figure()\n",
        "plt.plot(lstm_train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('LSTM Training Loss over Epochs')\n",
        "plt.show()\n",
        "\n",
        "# LSTM: Test Accuracy\n",
        "plt.figure()\n",
        "plt.plot(lstm_test_accuracies)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('LSTM Test Accuracy over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4DGn1xyAYYBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}